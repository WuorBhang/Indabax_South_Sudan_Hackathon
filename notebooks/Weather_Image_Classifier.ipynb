
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Weather Image Classifier\n",
    "\n",
    "This notebook demonstrates how to train and use an EfficientNetB0 model for weather classification.\n",
    "The model can classify images into 4 categories: Cloudy, Rain, Shine, and Sunrise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2", 
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "dataset_path = \"../indabax_south_sudan_intermediate\"\n",
    "train_dir = os.path.join(dataset_path, \"weather_dataset\")\n",
    "test_dir = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['Cloudy', 'Rain', 'Shine', 'Sunrise']\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(train_dir):\n",
    "    print(f\"Training directory found: {train_dir}\")\n",
    "    print(f\"Classes in dataset: {os.listdir(train_dir)}\")\n",
    "else:\n",
    "    print(f\"Training directory not found: {train_dir}\")\n",
    "    print(\"Please ensure your dataset is in the correct location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"Found classes: {class_names}\")\n",
    "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_dataset)}\")\n",
    "print(f\"Number of validation batches: {tf.data.experimental.cardinality(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(min(12, len(images))):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    \"\"\"Create EfficientNetB0 model for weather classification\"\"\"\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(len(class_names))\n",
    "print(f\"Model created with {len(class_names)} classes\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Apply augmentation and optimization\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Data augmentation and preprocessing applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, \n",
    "        restore_best_weights=True,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        patience=3,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        '../best_weather_model.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save('../weather_model.h5')\n",
    "print(\"Model saved as 'weather_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Learning rate plot (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "if 'lr' in history.history:\n",
    "    plt.plot(history.history['lr'], label='Learning Rate', marker='o')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Learning Rate\\nNot Available', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test dataset if available\n",
    "if os.path.exists(test_dir):\n",
    "    test_dataset = image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Test directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Image Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    \"\"\"Preprocess image for prediction\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Resize image to 224x224 (EfficientNetB0 input size)\n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    # Convert to RGB if needed\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array, img\n",
    "\n",
    "def predict_weather(img_path):\n",
    "    \"\"\"Predict weather from image path\"\"\"\n",
    "    processed_img, original_img = preprocess_image(img_path)\n",
    "    predictions = model.predict(processed_img, verbose=0)\n",
    "    \n",
    "    # Get predicted class and confidence\n",
    "    predicted_class_idx = np.argmax(predictions[0])\n",
    "    confidence = float(predictions[0][predicted_class_idx])\n",
    "    predicted_class = class_names[predicted_class_idx]\n",
    "    \n",
    "    # Get all predictions for display\n",
    "    all_predictions = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        all_predictions[class_name] = float(predictions[0][i])\n",
    "    \n",
    "    return predicted_class, confidence, all_predictions, original_img\n",
    "\n",
    "def plot_prediction(img_path):\n",
    "    \"\"\"Plot image with prediction results\"\"\"\n",
    "    predicted_class, confidence, all_predictions, img = predict_weather(img_path)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Display image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence:.2%}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display prediction probabilities\n",
    "    plt.subplot(1, 2, 2)\n",
    "    classes = list(all_predictions.keys())\n",
    "    probabilities = list(all_predictions.values())\n",
    "    \n",
    "    bars = plt.bar(classes, probabilities)\n",
    "    plt.title('Prediction Probabilities')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Highlight the predicted class\n",
    "    max_idx = probabilities.index(max(probabilities))\n",
    "    bars[max_idx].set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_class, confidence, all_predictions\n",
    "\n",
    "print(\"Prediction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Test Predictions (Example Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test prediction on a sample image\n",
    "# Replace 'path_to_your_image.jpg' with an actual image path\n",
    "\n",
    "# Uncomment and modify the path below to test with your own image\n",
    "# test_image_path = \"path_to_your_image.jpg\"\n",
    "# if os.path.exists(test_image_path):\n",
    "#     predicted_class, confidence, all_predictions = plot_prediction(test_image_path)\n",
    "#     print(f\"\\nPrediction Results:\")\n",
    "#     print(f\"Predicted Class: {predicted_class}\")\n",
    "#     print(f\"Confidence: {confidence:.2%}\")\n",
    "#     print(f\"All Predictions: {all_predictions}\")\n",
    "# else:\n",
    "#     print(\"Test image not found. Please provide a valid image path.\")\n",
    "\n",
    "print(\"To test predictions, uncomment the code above and provide an image path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Model Information and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final training results\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Number of Epochs Trained: {len(history.history['accuracy'])}\")\n",
    "print(f\"Model saved to: weather_model.h5\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"{i+1}. {layer.name}: {layer.__class__.__name__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
